{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "orange-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from quantities.units.area import D\n",
    "import pandas as pd\n",
    "import sklearn_crfsuite\n",
    "import spacy\n",
    "import quantities\n",
    "from quantities import units\n",
    "from quantities.unitquantity import UnitQuantity as UQ\n",
    "from sklearn_crfsuite import metrics\n",
    "from pprint import pprint\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "executive-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regular-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'QUANT' : 'Quantity', 'ME' : 'MeasuredEntity', 'MP' : 'MeasuredProperty'}\n",
    "lang_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "congressional-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_list = []\n",
    "\n",
    "train_raw_files = glob.glob(\"train/text/*.txt\")\n",
    "train_tsv_files = glob.glob(\"train/tsv/*.tsv\")\n",
    "test_raw_files = glob.glob(\"trial/txt/*.txt\")\n",
    "test_tsv_files = glob.glob(\"trial/tsv/*.tsv\")\n",
    "eval_raw_files = glob.glob(\"eval/text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arranged-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_to_df(raw_files):\n",
    "    \"\"\"converting raw training files to dataframes\"\"\"\n",
    "    # dict to be exported as dataframe\n",
    "    documents_of_interest = {\n",
    "        'document_name': [],\n",
    "        'sentence': [],\n",
    "        'entities': [],\n",
    "        'np': []\n",
    "    }\n",
    "    # filling the dict\n",
    "    for raw_file in raw_files:\n",
    "        with open(raw_file, \"r\") as file:\n",
    "            doc_name = file.name.split(\"/\")[2]\n",
    "            doc_name = doc_name.split('.')[0]\n",
    "            file_content = lang_model(file.read())\n",
    "            for sentence in file_content.sents:\n",
    "                sentence_pos_tags = [word.tag_ for word in sentence]\n",
    "                documents_of_interest['document_name'].append(doc_name)\n",
    "                documents_of_interest['sentence'].append(sentence)\n",
    "                entities = []\n",
    "                for measurement in sentence.ents:\n",
    "                    entities.append((measurement.label_,(measurement.start, measurement.end)))\n",
    "                documents_of_interest['entities'].append(\n",
    "                    entities\n",
    "                )\n",
    "\n",
    "                noun_phrases = []\n",
    "\n",
    "                for chunk in sentence.noun_chunks:\n",
    "                    noun_phrases.append((chunk.text, (chunk.start, chunk.end)))\n",
    "                documents_of_interest['np'].append(noun_phrases)\n",
    "                    \n",
    "        # break\n",
    "    dataframe = pd.DataFrame(\n",
    "        documents_of_interest,\n",
    "        columns=['document_name', 'sentence', 'entities', 'np'],\n",
    "    )\n",
    "    # pprint(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greatest-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_labels(text_dataframe, tsv_dataframe):\n",
    "    labels = []\n",
    "    for _, text_row in text_dataframe.iterrows():\n",
    "        sentence_tag_placeholders = []\n",
    "        for word in text_row['sentence']:\n",
    "            sentence_tag_placeholders.append(\n",
    "                ['O', (word.idx, (word.idx + len(word)))]\n",
    "            )\n",
    "        # O means not a quantity QUANT means quantity\n",
    "        document_name = text_row['document_name']\n",
    "#         print(document_name)\n",
    "        doc_id = tsv_dataframe['docId'] == document_name\n",
    "        for _, annot_row in tsv_dataframe[doc_id].iterrows():\n",
    "            annotType = annot_row['annotType']\n",
    "            if annotType == 'Qualifier':\n",
    "                continue\n",
    "\n",
    "            for key, value in tags.items():\n",
    "                if annotType == value:\n",
    "                    annotType = key\n",
    "                    break\n",
    "            \n",
    "            for i, item in enumerate(sentence_tag_placeholders):\n",
    "                if item[0] != 'O':\n",
    "                    continue\n",
    "                \n",
    "                if (annot_row['startOffset'] <= item[1][0] < annot_row['endOffset']) or (annot_row['startOffset'] < item[1][1] <= annot_row['endOffset']):\n",
    "                    sentence_tag_placeholders[i][0] = annotType\n",
    "    \n",
    "        labels.append([label for label, _ in sentence_tag_placeholders])\n",
    "                \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liberal-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_units() :\n",
    "    units_list = ['%'] # Add possible unit symbols\n",
    "    for key, value in units.__dict__.items():\n",
    "        if isinstance(value, UQ):\n",
    "            if key not in units_list :\n",
    "                units_list.append(key.lower())\n",
    "            if value.name not in units_list :\n",
    "                units_list.append(value.name.lower())\n",
    "        \n",
    "    return units_list\n",
    "\n",
    "def is_unit(token):\n",
    "    return token.lower_ in units_list or token.lemma_ in units_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressive-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_word(word, entities, nouns, length, pos):\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'lower': word.lower_,\n",
    "        'lemma': word.lemma_,\n",
    "        'upper': word.is_upper,\n",
    "        'title': word.is_title,\n",
    "        'digit': word.is_digit,\n",
    "        'numlike': word.like_num,\n",
    "        'unit': is_unit(word),\n",
    "        'postag': word.tag_,\n",
    "        'dep': word.dep_\n",
    "    }\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity[1][0] <= word.i < entity[1][1]:\n",
    "            features['entity'] = entity[0]\n",
    "            break\n",
    "    \n",
    "    for noun in nouns:\n",
    "        if noun[1][0] <= word.i < noun[1][1]:\n",
    "            features['np'] = list(noun[0])\n",
    "            break\n",
    "    \n",
    "    if pos >= 1 :\n",
    "        new_word = word.nbor(-1)\n",
    "        features.update({\n",
    "            '-1:lower': new_word.lower_,\n",
    "            '-1:lemma': new_word.lemma_,\n",
    "            '-1:upper': new_word.is_upper,\n",
    "            '-1:title': new_word.is_title,\n",
    "            '-1:digit': new_word.is_digit,\n",
    "            '-1:numlike': new_word.like_num,\n",
    "            '-1:unit': is_unit(new_word),\n",
    "            '-1:postag': new_word.tag_,\n",
    "            '-1:dep': new_word.dep_\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        features['start'] = True\n",
    "        \n",
    "        \n",
    "    if pos <= length-2 :\n",
    "        new_word = word.nbor(1)\n",
    "        features.update({\n",
    "            '+1:lower': new_word.lower_,\n",
    "            '+1:lemma': new_word.lemma_,\n",
    "            '+1:upper': new_word.is_upper,\n",
    "            '+1:title': new_word.is_title,\n",
    "            '+1:digit': new_word.is_digit,\n",
    "            '+1:numlike': new_word.like_num,\n",
    "            '+1:unit': is_unit(new_word),\n",
    "            '+1:postag': new_word.tag_,\n",
    "            '+1:dep': new_word.dep_\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        features['end'] = True\n",
    "    \n",
    "    window_2 = {'lower': [], 'lemma': [], 'pos': [], 'dep': []}\n",
    "    window_4 = {'lower': [], 'lemma': [], 'pos': [], 'dep': []}\n",
    "    for x in range(-4, 5):\n",
    "        if x == 0 or (pos + x < 0) or (pos + x > length - 1):\n",
    "            continue\n",
    "\n",
    "        window_word = word.nbor(x)\n",
    "\n",
    "        if x in range(-2, 3):\n",
    "            window_2['lower'].append(window_word.lower_)\n",
    "            window_2['pos'].append(window_word.pos_)\n",
    "            window_2['dep'].append(window_word.dep_)\n",
    "\n",
    "        window_4['lower'].append(window_word.lower_)\n",
    "        window_4['pos'].append(window_word.pos_)\n",
    "        window_4['dep'].append(window_word.dep_)\n",
    "\n",
    "    features.update({\n",
    "            'window_2_words:lower': window_2['lower'],\n",
    "            'window_2_words:pos': window_2['pos'],\n",
    "            'window_2_words:dep': window_2['dep'],\n",
    "            'window_4_words:lower': window_4['lower'],\n",
    "            'window_4_words:pos': window_4['pos'],\n",
    "            'window_4_words:dep': window_4['dep']\n",
    "    })\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inner-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_sentence(sentence, entities, nouns):\n",
    "    sentence_features = []\n",
    "    for i in range(0, len(sentence)) :\n",
    "        word_features = features_word(sentence[i], entities, nouns, len(sentence), i)\n",
    "        sentence_features.append(word_features)\n",
    "    return sentence_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atlantic-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(span, start, end):\n",
    "    if len(span) == 0:\n",
    "        return -1;\n",
    "    ind = -1\n",
    "    min_dist = 100000\n",
    "    for i in range(len(span)):\n",
    "        dist = 100000\n",
    "        if (start > span[i][1]) :\n",
    "            dist = start - span[i][1]\n",
    "        else :\n",
    "            dist = span[i][0] - end\n",
    "        if dist <= min_dist :\n",
    "            min_dist = dist\n",
    "            ind = i\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interstate-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tags(output_list, quant_list, mp_list, me_list, quantspan, mpspan, mespan):\n",
    "    \n",
    "    if (len(quant_list) <= 0) :\n",
    "        return output_list\n",
    "    \n",
    "    me_ind = [-1] * len(quant_list)\n",
    "    mp_ind = [-1] * len(quant_list)\n",
    "    \n",
    "#     me_mp_ind = [-1] * len(mp_list)\n",
    "    \n",
    "    for i in range(len(mpspan)):\n",
    "        closest = find_closest(quantspan, mpspan[i][0], mpspan[i][1])\n",
    "        if (closest == -1):\n",
    "            continue\n",
    "        mp_ind[closest] = i\n",
    "        \n",
    "#         closest_ent = find_closest(mespan, mpspan[i][0], mpspan[i][1])\n",
    "#         if (closest_ent == -1):\n",
    "#             continue\n",
    "#         me_mp_ind[i] = closest_ent\n",
    "    \n",
    "    for i in range(len(mespan)):\n",
    "        closest = find_closest(quantspan, mespan[i][0], mespan[i][1])\n",
    "        me_ind[closest] = i\n",
    "    \n",
    "    for i in range(len(quant_list)):\n",
    "#         quant_list[i][1] = annot_set_index\n",
    "#         quant_list[i][5] = annot_index\n",
    "        output_list.append(quant_list[i])\n",
    "    \n",
    "        if mp_ind[i] != -1:\n",
    "            mp_list[mp_ind[i]][1] = quant_list[i][1]\n",
    "            mp_list[mp_ind[i]][7] = '{\"HasQuantity\": \"' + str(quant_list[i][5]) + '\"}'\n",
    "#             mp_list[mp_ind[i]][5] = annot_index\n",
    "            output_list.append(mp_list[mp_ind[i]])\n",
    "            \n",
    "#             if me_mp_ind[mp_ind[i]] != -1 :\n",
    "#                 ent_ind = me_mp_ind[mp_ind[i]]\n",
    "#                 me_list[ent_ind][7] = '{\"HasProperty\": \"' + str( mp_list[mp_ind[i]][5]) + '\"}'\n",
    "#                 output_list.append(me_list[ent_ind])\n",
    "    \n",
    "        if me_ind[i] != -1:\n",
    "            me_list[me_ind[i]][1] = quant_list[i][1] \n",
    "            me_list[me_ind[i]][7] = '{\"HasQuantity\": \"' + str(quant_list[i][5]) + '\"}'\n",
    "            \n",
    "#             me_list[me_ind[i]][5] = annot_index\n",
    "            output_list.append(me_list[me_ind[i]])\n",
    "#         annot_set_index += 1\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "purple-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_tsv(text_dataframe, y_pred, dirname, units):\n",
    "    tsv_columns = [\n",
    "        \"docId\",\n",
    "        \"annotSet\",\n",
    "        \"annotType\",\n",
    "        \"startOffset\",\n",
    "        \"endOffset\",\n",
    "        \"annotId\",\n",
    "        \"text\",\n",
    "        \"other\"\n",
    "    ]\n",
    "    # save the results in appropriate format in a new dataframe\n",
    "    row_id = 0\n",
    "    output_list = []\n",
    "    prev_file = \"\"\n",
    "    annot_set_index = 1\n",
    "    annot_index = 1\n",
    "    for i, text_row in text_dataframe.iterrows():\n",
    "        quant_list = []\n",
    "        mp_list = []\n",
    "        me_list = []\n",
    "        quantspan = []\n",
    "        mpspan = []\n",
    "        mespan = []\n",
    "        file_name = text_row['document_name']\n",
    "        if i > 0 and file_name != prev_file:\n",
    "            result_df = pd.DataFrame(output_list, columns=tsv_columns)\n",
    "            result_df.to_csv(dirname + prev_file + '.tsv', sep=\"\\t\", index=False)\n",
    "            output_list = []\n",
    "            annot_set_index = 1\n",
    "            annot_index = 1\n",
    "\n",
    "        pred_pos = y_pred[row_id]\n",
    "        row_id += 1\n",
    "        sentence = text_row['sentence']\n",
    "        \n",
    "        word_ind = 0\n",
    "        while word_ind < len(pred_pos) :\n",
    "            if pred_pos[word_ind] != 'QUANT':\n",
    "                word_ind += 1\n",
    "                continue\n",
    "            \n",
    "            start_ind = word_ind\n",
    "            while word_ind < len(pred_pos) and pred_pos[word_ind] == 'QUANT':\n",
    "                word_ind += 1\n",
    "            end_ind = word_ind - 1\n",
    "            \n",
    "            quant_text = sentence.doc[sentence[start_ind].i : sentence[end_ind].i + 1]\n",
    "            unit = 'default'\n",
    "            flag = False\n",
    "            for i in range(start_ind, end_ind + 1):\n",
    "                if sentence[i].text in units:\n",
    "                    unit = sentence[i].text\n",
    "                    flag = True\n",
    "            if not flag:\n",
    "                unit = sentence[end_ind].text\n",
    "            quant_list.append([file_name, annot_set_index, tags['QUANT'], sentence[start_ind].idx, sentence[end_ind].idx + len(sentence[end_ind]), annot_index, quant_text, '{\"unit\": \"' + unit + '\"}'])\n",
    "            quantspan.append([sentence[start_ind].i, sentence[end_ind].i])\n",
    "            annot_set_index += 1\n",
    "            annot_index += 1\n",
    "            \n",
    "        word_ind = 0\n",
    "        while word_ind < len(pred_pos) :\n",
    "            if pred_pos[word_ind] != 'MP':\n",
    "                word_ind += 1\n",
    "                continue\n",
    "            \n",
    "            start_ind = word_ind\n",
    "            while word_ind < len(pred_pos) and pred_pos[word_ind] == 'MP':\n",
    "                word_ind += 1\n",
    "            end_ind = word_ind - 1\n",
    "            \n",
    "            mp_text = sentence.doc[sentence[start_ind].i : sentence[end_ind].i + 1]\n",
    "            mp_list.append([file_name, 0, tags['MP'], sentence[start_ind].idx, sentence[end_ind].idx + len(sentence[end_ind]), annot_index, mp_text, '{\"HasQuantity\": \"0\"}'])\n",
    "            mpspan.append([sentence[start_ind].i, sentence[end_ind].i])\n",
    "            annot_index += 1\n",
    "            \n",
    "        word_ind = 0\n",
    "        while word_ind < len(pred_pos) :\n",
    "            if pred_pos[word_ind] != 'ME':\n",
    "                word_ind += 1\n",
    "                continue\n",
    "            \n",
    "            start_ind = word_ind\n",
    "            while word_ind < len(pred_pos) and pred_pos[word_ind] == 'ME':\n",
    "                word_ind += 1\n",
    "            end_ind = word_ind - 1\n",
    "            \n",
    "            me_text = sentence.doc[sentence[start_ind].i : sentence[end_ind].i + 1]\n",
    "            me_list.append([file_name, 0, tags['ME'], sentence[start_ind].idx, sentence[end_ind].idx + len(sentence[end_ind]), annot_index, me_text, '{\"HasQuantity\": \"0\"}'])\n",
    "            mespan.append([sentence[start_ind].i, sentence[end_ind].i])\n",
    "            annot_index += 1\n",
    "        \n",
    "        output_list = merge_tags(output_list, quant_list, mp_list, me_list, quantspan, mpspan, mespan)\n",
    "#         print(\"Here\", annot_set_index)\n",
    "        prev_file = file_name\n",
    "        \n",
    "    result_df = pd.DataFrame(output_list, columns=tsv_columns)\n",
    "    result_df.to_csv(dirname + prev_file + '.tsv', sep=\"\\t\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "julian-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_predictions_to_tsv(train_text_dataframe, y_train, 'sample_tsv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "chemical-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_list = get_units()\n",
    "\n",
    "train_text_dataframe = raw_text_to_df(train_raw_files)\n",
    "train_text_dataframe.to_csv(\"./CSV/train_text_dataframe.csv\")\n",
    "\n",
    "each_file_df = []\n",
    "for tsv_file in train_tsv_files:\n",
    "    each_file_df.append(pd.read_csv(tsv_file, sep=\"\\t\", header=0))\n",
    "\n",
    "train_tsv_dataframe = pd.concat(each_file_df)\n",
    "train_tsv_dataframe.to_csv(\"./CSV/train_tsv_dataframe.csv\")\n",
    "\n",
    "test_text_dataframe = raw_text_to_df(test_raw_files)\n",
    "test_text_dataframe.to_csv(\"./CSV/test_text_dataframe.csv\")\n",
    "\n",
    "each_file_test = []\n",
    "for tsv_file in test_tsv_files:\n",
    "    each_file_test.append(pd.read_csv(tsv_file, sep=\"\\t\", header=0))\n",
    "\n",
    "test_tsv_dataframe = pd.concat(each_file_test)\n",
    "test_tsv_dataframe.to_csv(\"./CSV/test_tsv_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "canadian-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for _, row in train_text_dataframe.iterrows() :\n",
    "    features = features_sentence(row['sentence'], row['entities'], row['np'])\n",
    "    X_train.append(features)\n",
    "    \n",
    "y_train = get_text_labels(train_text_dataframe, train_tsv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "constant-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for _, row in test_text_dataframe.iterrows() :\n",
    "    features = features_sentence(row['sentence'], row['entities'], row['np'])\n",
    "    X_test.append(features)\n",
    "    \n",
    "y_test = get_text_labels(test_text_dataframe, test_tsv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "round-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train + X_test\n",
    "y_train = y_train + y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "graduate-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text_dataframe = raw_text_to_df(eval_raw_files)\n",
    "X_eval = []\n",
    "for _, row in eval_text_dataframe.iterrows() :\n",
    "    features = features_sentence(row['sentence'], row['entities'], row['np'])\n",
    "    X_eval.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interesting-house",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "still-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "yellow-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_eval)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "occupied-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions_to_tsv(eval_text_dataframe, y_pred, 'results_task3/', units_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "changed-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = sklearn_crfsuite.CRF(\n",
    "#     algorithm='lbfgs',\n",
    "#     max_iterations=100,\n",
    "#     all_possible_transitions=True\n",
    "# )\n",
    "\n",
    "# params_space = {\n",
    "#     'c1': scipy.stats.expon(scale=2),\n",
    "#     'c2': scipy.stats.expon(scale=2), \n",
    "# }\n",
    "\n",
    "# f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "#                         average='weighted', labels=labels)\n",
    "\n",
    "# rs = RandomizedSearchCV(crf, params_space,\n",
    "#                         cv=3,\n",
    "#                         verbose=1,\n",
    "#                         n_jobs=-1,\n",
    "#                         n_iter=50,\n",
    "#                         scoring=f1_scorer)\n",
    "\n",
    "# rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = rs.best_estimator_\n",
    "# y_pred = crf.predict(X_eval)\n",
    "# print(metrics.flat_classification_report(\n",
    "#     y_test, y_pred, labels=sorted_labels, digits=3\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_predictions_to_tsv(eval_text_dataframe, y_pred, 'results_task3_opt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
