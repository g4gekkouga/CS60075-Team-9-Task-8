{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "recent-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from quantities.units.area import D\n",
    "import pandas as pd\n",
    "import sklearn_crfsuite\n",
    "import spacy\n",
    "import quantities\n",
    "from quantities import units\n",
    "from quantities.unitquantity import UnitQuantity as UQ\n",
    "from sklearn_crfsuite import metrics\n",
    "from pprint import pprint\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "worldwide-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "composed-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'QUANT' : 'Quantity', 'ME' : 'MeasuredEntity', 'MP' : 'MeasuredProperty'}\n",
    "lang_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "iraqi-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_list = []\n",
    "\n",
    "train_raw_files = glob.glob(\"train/text/*.txt\")\n",
    "train_tsv_files = glob.glob(\"train/tsv/*.tsv\")\n",
    "test_raw_files = glob.glob(\"trial/txt/*.txt\")\n",
    "test_tsv_files = glob.glob(\"trial/tsv/*.tsv\")\n",
    "eval_raw_files = glob.glob(\"eval/text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "english-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_to_df(raw_files):\n",
    "    \"\"\"converting raw training files to dataframes\"\"\"\n",
    "    # dict to be exported as dataframe\n",
    "    documents_of_interest = {\n",
    "        'document_name': [],\n",
    "        'sentence': [],\n",
    "        'entities': [],\n",
    "        'np': []\n",
    "    }\n",
    "    # filling the dict\n",
    "    for raw_file in raw_files:\n",
    "        with open(raw_file, \"r\") as file:\n",
    "            doc_name = file.name.split(\"/\")[2]\n",
    "            doc_name = doc_name.split('.')[0]\n",
    "            file_content = lang_model(file.read())\n",
    "            for sentence in file_content.sents:\n",
    "                sentence_pos_tags = [word.tag_ for word in sentence]\n",
    "                documents_of_interest['document_name'].append(doc_name)\n",
    "                documents_of_interest['sentence'].append(sentence)\n",
    "                entities = []\n",
    "                for measurement in sentence.ents:\n",
    "                    entities.append((measurement.label_,(measurement.start, measurement.end)))\n",
    "                documents_of_interest['entities'].append(\n",
    "                    entities\n",
    "                )\n",
    "\n",
    "                noun_phrases = []\n",
    "\n",
    "                for chunk in sentence.noun_chunks:\n",
    "                    noun_phrases.append((chunk.text, (chunk.start, chunk.end)))\n",
    "                documents_of_interest['np'].append(noun_phrases)\n",
    "                    \n",
    "        # break\n",
    "    dataframe = pd.DataFrame(\n",
    "        documents_of_interest,\n",
    "        columns=['document_name', 'sentence', 'entities', 'np'],\n",
    "    )\n",
    "    # pprint(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "nonprofit-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_labels(text_dataframe, tsv_dataframe):\n",
    "    labels = []\n",
    "    for _, text_row in text_dataframe.iterrows():\n",
    "        sentence_tag_placeholders = []\n",
    "        for word in text_row['sentence']:\n",
    "            sentence_tag_placeholders.append(\n",
    "                ['O', (word.idx, (word.idx + len(word)))]\n",
    "            )\n",
    "        # O means not a quantity QUANT means quantity\n",
    "        document_name = text_row['document_name']\n",
    "#         print(document_name)\n",
    "        doc_id = tsv_dataframe['docId'] == document_name\n",
    "        for _, annot_row in tsv_dataframe[doc_id].iterrows():\n",
    "            annotType = annot_row['annotType']\n",
    "            if annotType == 'Qualifier':\n",
    "                continue\n",
    "\n",
    "            for key, value in tags.items():\n",
    "                if annotType == value:\n",
    "                    annotType = key\n",
    "                    break\n",
    "            \n",
    "            for i, item in enumerate(sentence_tag_placeholders):\n",
    "                if item[0] != 'O':\n",
    "                    continue\n",
    "                \n",
    "                if (annot_row['startOffset'] <= item[1][0] < annot_row['endOffset']) or (annot_row['startOffset'] < item[1][1] <= annot_row['endOffset']):\n",
    "                    sentence_tag_placeholders[i][0] = annotType\n",
    "    \n",
    "        labels.append([label for label, _ in sentence_tag_placeholders])\n",
    "                \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "boolean-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_units() :\n",
    "    units_list = ['%'] # Add possible unit symbols\n",
    "    for key, value in units.__dict__.items():\n",
    "        if isinstance(value, UQ):\n",
    "            if key not in units_list :\n",
    "                units_list.append(key.lower())\n",
    "            if value.name not in units_list :\n",
    "                units_list.append(value.name.lower())\n",
    "        \n",
    "    return units_list\n",
    "\n",
    "def is_unit(token):\n",
    "    return token.lower_ in units_list or token.lemma_ in units_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dominant-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_word(word, entities, nouns, length, pos):\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'lower': word.lower_,\n",
    "        'lemma': word.lemma_,\n",
    "        'upper': word.is_upper,\n",
    "        'title': word.is_title,\n",
    "        'digit': word.is_digit,\n",
    "        'numlike': word.like_num,\n",
    "        'unit': is_unit(word),\n",
    "        'postag': word.tag_,\n",
    "        'dep': word.dep_\n",
    "    }\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity[1][0] <= word.i < entity[1][1]:\n",
    "            features['entity'] = entity[0]\n",
    "            break\n",
    "    \n",
    "    for noun in nouns:\n",
    "        if noun[1][0] <= word.i < noun[1][1]:\n",
    "            features['np'] = list(noun[0])\n",
    "            break\n",
    "    \n",
    "    if pos >= 1 :\n",
    "        new_word = word.nbor(-1)\n",
    "        features.update({\n",
    "            '-1:lower': new_word.lower_,\n",
    "            '-1:lemma': new_word.lemma_,\n",
    "            '-1:upper': new_word.is_upper,\n",
    "            '-1:title': new_word.is_title,\n",
    "            '-1:digit': new_word.is_digit,\n",
    "            '-1:numlike': new_word.like_num,\n",
    "            '-1:unit': is_unit(new_word),\n",
    "            '-1:postag': new_word.tag_,\n",
    "            '-1:dep': new_word.dep_\n",
    "        })\n",
    "        \n",
    "        \n",
    "    if pos <= length-2 :\n",
    "        new_word = word.nbor(1)\n",
    "        features.update({\n",
    "            '+1:lower': new_word.lower_,\n",
    "            '+1:lemma': new_word.lemma_,\n",
    "            '+1:upper': new_word.is_upper,\n",
    "            '+1:title': new_word.is_title,\n",
    "            '+1:digit': new_word.is_digit,\n",
    "            '+1:numlike': new_word.like_num,\n",
    "            '+1:unit': is_unit(new_word),\n",
    "            '+1:postag': new_word.tag_,\n",
    "            '+1:dep': new_word.dep_\n",
    "        })\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "marine-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_sentence(sentence, entities, nouns):\n",
    "    sentence_features = []\n",
    "    for i in range(0, len(sentence)) :\n",
    "        word_features = features_word(sentence[i], entities, nouns, len(sentence), i)\n",
    "        sentence_features.append(word_features)\n",
    "    return sentence_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "derived-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_tsv(text_dataframe, y_pred, dirname):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "entertaining-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(quantspan, start, end):\n",
    "    ind = -1\n",
    "    min_dist = 100000\n",
    "    for i in range(len(quantspan)):\n",
    "        dist = 100000\n",
    "        if (start > quantspan[i][1]) :\n",
    "            dist = start - quantspan[i][1]\n",
    "        else :\n",
    "            dist = quantspan[i][0] - end\n",
    "        if dist <= min_dist :\n",
    "            min_dist = dist\n",
    "            ind = i\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "literary-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tags(output_list, quant_list, mp_list, me_list, quantspan, mpspan, mespan):\n",
    "    \n",
    "    if (len(quant_list) <= 0) :\n",
    "        return output_list\n",
    "    \n",
    "    me_ind = [-1] * len(quant_list)\n",
    "    mp_ind = [-1] * len(quant_list)\n",
    "    \n",
    "    for i in range(len(mpspan)):\n",
    "        closest = find_closest(quantspan, mpspan[i][0], mpspan[i][1])\n",
    "        mp_ind[closest] = i\n",
    "    \n",
    "    for i in range(len(mespan)):\n",
    "        closest = find_closest(quantspan, mespan[i][0], mespan[i][1])\n",
    "#         print(len(quant_list), '--', closest)\n",
    "        me_ind[closest] = i\n",
    "    \n",
    "    for i in range(len(quant_list)):\n",
    "#         quant_list[i][1] = annot_set_index\n",
    "#         quant_list[i][5] = annot_index\n",
    "        output_list.append(quant_list[i])\n",
    "        if mp_ind[i] != -1:\n",
    "            mp_list[mp_ind[i]][1] = quant_list[i][1]\n",
    "#             mp_list[mp_ind[i]][5] = annot_index\n",
    "            output_list.append(mp_list[mp_ind[i]])\n",
    "        if me_ind[i] != -1:\n",
    "            me_list[me_ind[i]][1] = quant_list[i][1]\n",
    "#             me_list[me_ind[i]][5] = annot_index\n",
    "            output_list.append(me_list[me_ind[i]])\n",
    "#         annot_set_index += 1\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "grand-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_tsv(text_dataframe, y_pred, dirname):\n",
    "    tsv_columns = [\n",
    "        \"docId\",\n",
    "        \"annotSet\",\n",
    "        \"annotType\",\n",
    "        \"startOffset\",\n",
    "        \"endOffset\",\n",
    "        \"annotId\",\n",
    "        \"text\",\n",
    "        \"other\"\n",
    "    ]\n",
    "    # save the results in appropriate format in a new dataframe\n",
    "    row_id = 0\n",
    "    output_list = []\n",
    "    prev_file = \"\"\n",
    "    annot_set_index = 1\n",
    "    annot_index = 1\n",
    "    for i, text_row in text_dataframe.iterrows():\n",
    "        quant_list = []\n",
    "        mp_list = []\n",
    "        me_list = []\n",
    "        quantspan = []\n",
    "        mpspan = []\n",
    "        mespan = []\n",
    "        file_name = text_row['document_name']\n",
    "        if i > 0 and file_name != prev_file:\n",
    "            result_df = pd.DataFrame(output_list, columns=tsv_columns)\n",
    "            result_df.to_csv(dirname + prev_file + '.tsv', sep=\"\\t\", index=False)\n",
    "            output_list = []\n",
    "            annot_set_index = 1\n",
    "            annot_index = 1\n",
    "\n",
    "        pred_pos = y_pred[row_id]\n",
    "        row_id += 1\n",
    "        sentence = text_row['sentence']\n",
    "        \n",
    "        word_ind = 0\n",
    "        while word_ind < len(pred_pos) :\n",
    "            if pred_pos[word_ind] != 'QUANT':\n",
    "                word_ind += 1\n",
    "                continue\n",
    "            \n",
    "            start_ind = word_ind\n",
    "            while word_ind < len(pred_pos) and pred_pos[word_ind] == 'QUANT':\n",
    "                word_ind += 1\n",
    "            end_ind = word_ind - 1\n",
    "            \n",
    "            quant_text = sentence.doc[sentence[start_ind].i : sentence[end_ind].i + 1]\n",
    "            quant_list.append([file_name, annot_set_index, tags['QUANT'], sentence[start_ind].idx, sentence[end_ind].idx + len(sentence[end_ind]), annot_index, quant_text, \"\"])\n",
    "            quantspan.append([sentence[start_ind].i, sentence[end_ind].i])\n",
    "            annot_set_index += 1\n",
    "            annot_index += 1\n",
    "            \n",
    "        word_ind = 0\n",
    "        while word_ind < len(pred_pos) :\n",
    "            if pred_pos[word_ind] != 'MP':\n",
    "                word_ind += 1\n",
    "                continue\n",
    "            \n",
    "            start_ind = word_ind\n",
    "            while word_ind < len(pred_pos) and pred_pos[word_ind] == 'MP':\n",
    "                word_ind += 1\n",
    "            end_ind = word_ind - 1\n",
    "            \n",
    "            mp_text = sentence.doc[sentence[start_ind].i : sentence[end_ind].i + 1]\n",
    "            mp_list.append([file_name, 0, tags['MP'], sentence[start_ind].idx, sentence[end_ind].idx + len(sentence[end_ind]), annot_index, mp_text, \"\"])\n",
    "            mpspan.append([sentence[start_ind].i, sentence[end_ind].i])\n",
    "            annot_index += 1\n",
    "            \n",
    "        word_ind = 0\n",
    "        while word_ind < len(pred_pos) :\n",
    "            if pred_pos[word_ind] != 'ME':\n",
    "                word_ind += 1\n",
    "                continue\n",
    "            \n",
    "            start_ind = word_ind\n",
    "            while word_ind < len(pred_pos) and pred_pos[word_ind] == 'ME':\n",
    "                word_ind += 1\n",
    "            end_ind = word_ind - 1\n",
    "            \n",
    "            me_text = sentence.doc[sentence[start_ind].i : sentence[end_ind].i + 1]\n",
    "            me_list.append([file_name, 0, tags['ME'], sentence[start_ind].idx, sentence[end_ind].idx + len(sentence[end_ind]), annot_index, me_text, \"\"])\n",
    "            mespan.append([sentence[start_ind].i, sentence[end_ind].i])\n",
    "            annot_index += 1\n",
    "        \n",
    "        output_list = merge_tags(output_list, quant_list, mp_list, me_list, quantspan, mpspan, mespan)\n",
    "#         print(\"Here\", annot_set_index)\n",
    "        prev_file = file_name\n",
    "        \n",
    "    result_df = pd.DataFrame(output_list, columns=tsv_columns)\n",
    "    result_df.to_csv(dirname + prev_file + '.tsv', sep=\"\\t\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "streaming-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_predictions_to_tsv(train_text_dataframe, y_train, 'sample_tsv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "passive-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_list = get_units()\n",
    "\n",
    "train_text_dataframe = raw_text_to_df(train_raw_files)\n",
    "train_text_dataframe.to_csv(\"./CSV/train_text_dataframe.csv\")\n",
    "\n",
    "each_file_df = []\n",
    "for tsv_file in train_tsv_files:\n",
    "    each_file_df.append(pd.read_csv(tsv_file, sep=\"\\t\", header=0))\n",
    "\n",
    "train_tsv_dataframe = pd.concat(each_file_df)\n",
    "train_tsv_dataframe.to_csv(\"./CSV/train_tsv_dataframe.csv\")\n",
    "\n",
    "test_text_dataframe = raw_text_to_df(test_raw_files)\n",
    "test_text_dataframe.to_csv(\"./CSV/test_text_dataframe.csv\")\n",
    "\n",
    "each_file_test = []\n",
    "for tsv_file in test_tsv_files:\n",
    "    each_file_test.append(pd.read_csv(tsv_file, sep=\"\\t\", header=0))\n",
    "\n",
    "test_tsv_dataframe = pd.concat(each_file_test)\n",
    "test_tsv_dataframe.to_csv(\"./CSV/test_tsv_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "hazardous-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for _, row in train_text_dataframe.iterrows() :\n",
    "    features = features_sentence(row['sentence'], row['entities'], row['np'])\n",
    "    X_train.append(features)\n",
    "    \n",
    "y_train = get_text_labels(train_text_dataframe, train_tsv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "balanced-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for _, row in test_text_dataframe.iterrows() :\n",
    "    features = features_sentence(row['sentence'], row['entities'], row['np'])\n",
    "    X_test.append(features)\n",
    "    \n",
    "y_test = get_text_labels(test_text_dataframe, test_tsv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "valuable-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train + X_test\n",
    "y_train = y_train + y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "alternate-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text_dataframe = raw_text_to_df(eval_raw_files)\n",
    "X_eval = []\n",
    "for _, row in eval_text_dataframe.iterrows() :\n",
    "    features = features_sentence(row['sentence'], row['entities'], row['np'])\n",
    "    X_eval.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "round-template",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "capital-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "organized-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_eval)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "traditional-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions_to_tsv(eval_text_dataframe, y_pred, 'results_task3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "lasting-guidance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-62b5a6faceae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         scoring=f1_scorer)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1529\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# When workers are killed in such a brutal manner, they cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexecutor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.5), \n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_eval)\n",
    "# print(metrics.flat_classification_report(\n",
    "#     y_test, y_pred, labels=sorted_labels, digits=3\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions_to_tsv(eval_text_dataframe, y_pred, 'results_task3_opt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-serve",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
